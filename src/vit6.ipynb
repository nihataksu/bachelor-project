{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import os \n",
    "import sys\n",
    "\n",
    "import importlib\n",
    "import hivit\n",
    "importlib.reload(hivit)\n",
    "\n",
    "from hivit.parameters import Parameters\n",
    "from hivit.hilbert_potitional_embedding import PatchEmbeddingHilbertPositionalEmbedding\n",
    "from hivit.no_positional_embedding import PatchEmbeddingNoPositionalEmbedding\n",
    "from hivit.learned_positional_embedding import PatchEmbeddingLearnedPositionalEmbedding\n",
    "from hivit.cut_out import Cutout\n",
    "from hivit.training import training_loop\n",
    "from hivit.test_model import test_model\n",
    "from datetime import datetime\n",
    "from hivit.cifar10_dataloader import cifar10_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available(): \n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"Metal or CUDA is not found!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RANDOM_SEED': 42, 'BATCH_SIZE': 512, 'EPOCHS': 1, 'PATIENCE': 20, 'LEARNING_RATE': 0.001, 'NUM_CLASSES': 10, 'PATCH_SIZE': 4, 'IMAGE_SIZE': 32, 'IN_CHANNELS': 3, 'NUM_HEADS': 12, 'DROPOUT': 0.3, 'HIDDEN_DIM': 1024, 'ADAM_WEIGHT_DECAY': 0, 'ADAM_BETAS': (0.9, 0.999), 'ACTIVATION': 'gelu', 'NUM_ENCODERS': 12, 'EMBEDING_DIMENTION': 48, 'NUM_PATCHES': 64, 'DATASET_ROOT': './data'}\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "working_folder = f\"experiment_results/cifar10_{current_time}\"\n",
    "os.makedirs(working_folder, exist_ok=True)\n",
    "\n",
    "parameters = Parameters()\n",
    "\n",
    "parameters.print()\n",
    "\n",
    "parameters.save_to_json(os.path.join(working_folder, \"parameters.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = cifar10_dataloader(parameters.DATASET_ROOT, parameters.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nihat/mambaforge/envs/bachelor-project/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      " 72%|███████▏  | 63/88 [03:04<01:10,  2.83s/it]"
     ]
    }
   ],
   "source": [
    "learned_postional_embedding = PatchEmbeddingLearnedPositionalEmbedding(parameters.EMBEDING_DIMENTION, parameters.PATCH_SIZE, parameters.NUM_PATCHES, parameters.DROPOUT, parameters.IN_CHANNELS)\n",
    "model_learned = training_loop(\n",
    "    working_folder,\n",
    "    \"learned_training\",\n",
    "    learned_postional_embedding, \n",
    "    'best_model_learned_embedding.pt', \n",
    "    device=device, \n",
    "    train_dataloader=train_dataloader, \n",
    "    val_dataloader=val_dataloader, \n",
    "    EPOCHS=parameters.EPOCHS,\n",
    "    PATIENCE=parameters.PATIENCE,\n",
    "    LEARNING_RATE=parameters.LEARNING_RATE,\n",
    "    NUM_CLASSES=parameters.NUM_CLASSES,\n",
    "    PATCH_SIZE=parameters.PATCH_SIZE,\n",
    "    IMAGE_SIZE=parameters.IMAGE_SIZE,\n",
    "    IN_CHANNELS=parameters.IN_CHANNELS,\n",
    "    NUM_HEADS=parameters.NUM_HEADS,\n",
    "    DROPOUT=parameters.DROPOUT,\n",
    "    HIDDEN_DIM=parameters.HIDDEN_DIM,\n",
    "    ADAM_WEIGHT_DECAY=parameters.ADAM_WEIGHT_DECAY,\n",
    "    ADAM_BETAS=parameters.ADAM_BETAS,\n",
    "    ACTIVATION=parameters.ACTIVATION,\n",
    "    NUM_ENCODERS=parameters.NUM_ENCODERS,\n",
    "    EMBEDING_DIMENTION=parameters.EMBEDING_DIMENTION,\n",
    "    NUM_PATCHES=parameters.NUM_PATCHES)\n",
    "test_model(model_learned, \"learned_test\", 'best_model_learned_embedding.pt', device=device, test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hilbert_embedding = PatchEmbeddingHilbertPositionalEmbedding(parameters.EMBEDING_DIMENTION, parameters.PATCH_SIZE, parameters.NUM_PATCHES, parameters.DROPOUT, parameters.IN_CHANNELS, parameters.IMAGE_SIZE)\n",
    "model_hilbert = training_loop(\n",
    "    working_folder,\n",
    "    \"hilbert_training\",\n",
    "    hilbert_embedding, \n",
    "    'hilbert_embedding.pt',\n",
    "    device=device, \n",
    "    train_dataloader=train_dataloader, \n",
    "    val_dataloader=val_dataloader, \n",
    "    EPOCHS=parameters.EPOCHS,\n",
    "    PATIANCE=parameters.PATIANCE,\n",
    "    LEARNING_RATE=parameters.LEARNING_RATE,\n",
    "    NUM_CLASSES=parameters.NUM_CLASSES,\n",
    "    PATCH_SIZE=parameters.PATCH_SIZE,\n",
    "    IMAGE_SIZE=parameters.IMAGE_SIZE,\n",
    "    IN_CHANNELS=parameters.IN_CHANNELS,\n",
    "    NUM_HEADS=parameters.NUM_HEADS,\n",
    "    DROPOUT=parameters.DROPOUT,\n",
    "    HIDDEN_DIM=parameters.HIDDEN_DIM,\n",
    "    ADAM_WEIGHT_DECAY=parameters.ADAM_WEIGHT_DECAY,\n",
    "    ADAM_BETAS=parameters.ADAM_BETAS,\n",
    "    ACTIVATION=parameters.ACTIVATION,\n",
    "    NUM_ENCODERS=parameters.NUM_ENCODERS,\n",
    "    EMBEDING_DIMENTION=parameters.EMBEDING_DIMENTION,\n",
    "    NUM_PATCHES=parameters.NUM_PATCHES)\n",
    "test_model(model_hilbert, \"hilbert_test\", 'hilbert_embedding.pt', device=device, test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_postional_embedding = PatchEmbeddingNoPositionalEmbedding(parameters.EMBEDING_DIMENTION, parameters.PATCH_SIZE, parameters.NUM_PATCHES, parameters.DROPOUT, parameters.IN_CHANNELS)\n",
    "model_no_embedding = training_loop(\n",
    "    working_folder,\n",
    "    \"no_positional_embedding_training\"\n",
    "    no_postional_embedding, \n",
    "    'best_model_no_embedding.pt', \n",
    "    device=device, \n",
    "    train_dataloader=train_dataloader, \n",
    "    val_dataloader=val_dataloader, \n",
    "    EPOCHS=parameters.EPOCHS,\n",
    "    PATIANCE=parameters.PATIANCE,\n",
    "    LEARNING_RATE=parameters.LEARNING_RATE,\n",
    "    NUM_CLASSES=parameters.NUM_CLASSES,\n",
    "    PATCH_SIZE=parameters.PATCH_SIZE,\n",
    "    IMAGE_SIZE=parameters.IMAGE_SIZE,\n",
    "    IN_CHANNELS=parameters.IN_CHANNELS,\n",
    "    NUM_HEADS=parameters.NUM_HEADS,\n",
    "    DROPOUT=parameters.DROPOUT,\n",
    "    HIDDEN_DIM=parameters.HIDDEN_DIM,\n",
    "    ADAM_WEIGHT_DECAY=parameters.ADAM_WEIGHT_DECAY,\n",
    "    ADAM_BETAS=parameters.ADAM_BETAS,\n",
    "    ACTIVATION=parameters.ACTIVATION,\n",
    "    NUM_ENCODERS=parameters.NUM_ENCODERS,\n",
    "    EMBEDING_DIMENTION=parameters.EMBEDING_DIMENTION,\n",
    "    NUM_PATCHES=parameters.NUM_PATCHES)\n",
    "test_model(model_no_embedding, \"no_positional_embedding_test\",'best_model_no_embedding.pt', device=device, test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
